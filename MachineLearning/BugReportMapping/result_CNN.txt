ssh://zchao@tensor.cs.ohio.edu:22/home/zchao/anaconda3/envs/tensorflow/bin/python3.5 -u /home/zchao/github/UltimatePython/MachineLearning/BugReportMapping/BugReportMapping_CNN.py
[nltk_data] Downloading package stopwords to /home/zchao/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
0 lines processed.

100 lines processed.

200 lines processed.

300 lines processed.

400 lines processed.

500 lines processed.

600 lines processed.

700 lines processed.

800 lines processed.

900 lines processed.

1000 lines processed.

1100 lines processed.

1200 lines processed.

1300 lines processed.

1400 lines processed.

1500 lines processed.

1600 lines processed.

1700 lines processed.

1800 lines processed.

1900 lines processed.

2000 lines processed.

2100 lines processed.

2200 lines processed.

2300 lines processed.

2400 lines processed.

2500 lines processed.

2600 lines processed.

2700 lines processed.

2800 lines processed.

2900 lines processed.

3000 lines processed.

3100 lines processed.

3200 lines processed.

3300 lines processed.

3400 lines processed.

3500 lines processed.

3600 lines processed.

3700 lines processed.

3800 lines processed.

3900 lines processed.

4000 lines processed.

4100 lines processed.

4200 lines processed.

4300 lines processed.

4400 lines processed.

4500 lines processed.

4600 lines processed.

4700 lines processed.

4800 lines processed.

4900 lines processed.

5000 lines processed.

5100 lines processed.

5200 lines processed.

5300 lines processed.

5400 lines processed.

5500 lines processed.

5600 lines processed.

5700 lines processed.

5800 lines processed.

5900 lines processed.

6000 lines processed.

6100 lines processed.

6200 lines processed.

6300 lines processed.

6400 lines processed.

W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:06:00.0
Total memory: 7.92GiB
Free memory: 7.81GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3d6aee0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties:
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:05:00.0
Total memory: 7.91GiB
Free memory: 7.35GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:06:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080, pci bus id: 0000:05:00.0)
Writing to /home/zchao/github/UltimatePython/MachineLearning/BugReportMapping/runs/1489392957

2017-03-13T04:16:35.749415: step 0, loss 60.6012, acc 0
2017-03-13T04:17:13.772748: step 1, loss 64.7228, acc 0.00390625
2017-03-13T04:17:51.934569: step 2, loss 53.2771, acc 0.136719
2017-03-13T04:18:30.139859: step 3, loss 68.3924, acc 0.375
2017-03-13T04:19:08.393134: step 4, loss 55.1653, acc 0.429688
2017-03-13T04:19:46.768962: step 5, loss 49.0908, acc 0.394531
2017-03-13T04:20:25.371635: step 6, loss 51.1377, acc 0.492188
2017-03-13T04:21:03.145416: step 7, loss 45.6013, acc 0.492188
2017-03-13T04:21:41.794148: step 8, loss 44.0902, acc 0.449219
2017-03-13T04:22:19.716927: step 9, loss 49.9089, acc 0.457031
2017-03-13T04:22:58.286836: step 10, loss 55.3621, acc 0.484375
2017-03-13T04:23:36.817259: step 11, loss 50.7414, acc 0.453125
2017-03-13T04:24:15.459955: step 12, loss 67.7128, acc 0.441406
2017-03-13T04:24:54.125248: step 13, loss 70.443, acc 0.464844
2017-03-13T04:25:32.471039: step 14, loss 72.5868, acc 0.414062
2017-03-13T04:26:11.064548: step 15, loss 63.8528, acc 0.421875
2017-03-13T04:26:49.926422: step 16, loss 59.0929, acc 0.476562
2017-03-13T04:27:27.787096: step 17, loss 66.3101, acc 0.507812
2017-03-13T04:28:06.645827: step 18, loss 97.9276, acc 0.46875
2017-03-13T04:28:44.782111: step 19, loss 109.126, acc 0.476562
2017-03-13T04:28:56.072361: step 20, loss 53.9378, acc 0.407895
2017-03-13T04:29:34.577619: step 21, loss 77.2231, acc 0.46875
2017-03-13T04:30:13.557884: step 22, loss 75.3769, acc 0.457031
2017-03-13T04:30:51.652645: step 23, loss 69.538, acc 0.378906
2017-03-13T04:31:29.464168: step 24, loss 106.6, acc 0.394531
2017-03-13T04:32:07.638873: step 25, loss 128.497, acc 0.390625
2017-03-13T04:32:46.075160: step 26, loss 78.1354, acc 0.398438
2017-03-13T04:33:24.319594: step 27, loss 83.7899, acc 0.359375
2017-03-13T04:34:02.516510: step 28, loss 93.424, acc 0.300781
2017-03-13T04:34:41.336592: step 29, loss 78.528, acc 0.277344
2017-03-13T04:35:19.807365: step 30, loss 122.71, acc 0.320312
2017-03-13T04:35:58.099259: step 31, loss 132.573, acc 0.308594
2017-03-13T04:36:36.990627: step 32, loss 91.7242, acc 0.382812
2017-03-13T04:37:14.920799: step 33, loss 108.859, acc 0.320312
2017-03-13T04:37:53.062652: step 34, loss 169.31, acc 0.359375
2017-03-13T04:38:31.837954: step 35, loss 187.556, acc 0.328125
2017-03-13T04:39:09.817116: step 36, loss 110.341, acc 0.316406
2017-03-13T04:39:48.019210: step 37, loss 112.837, acc 0.382812
2017-03-13T04:40:27.013480: step 38, loss 147.725, acc 0.425781
2017-03-13T04:41:05.140650: step 39, loss 132.799, acc 0.355469
2017-03-13T04:41:43.320446: step 40, loss 143.79, acc 0.421875
2017-03-13T04:41:55.038847: step 41, loss 157.849, acc 0.407895
2017-03-13T04:42:33.765226: step 42, loss 155.974, acc 0.414062
2017-03-13T04:43:12.412700: step 43, loss 169.135, acc 0.328125
2017-03-13T04:43:50.931912: step 44, loss 173.165, acc 0.429688
2017-03-13T04:44:29.168019: step 45, loss 222.471, acc 0.476562
2017-03-13T04:45:07.744300: step 46, loss 186.037, acc 0.316406
2017-03-13T04:45:45.990163: step 47, loss 254.587, acc 0.421875
2017-03-13T04:46:24.318467: step 48, loss 204.563, acc 0.355469
2017-03-13T04:47:02.566651: step 49, loss 229.295, acc 0.359375
2017-03-13T04:47:40.690180: step 50, loss 258.069, acc 0.355469
2017-03-13T04:48:19.360090: step 51, loss 332.732, acc 0.367188
2017-03-13T04:48:57.648349: step 52, loss 246.729, acc 0.390625
2017-03-13T04:49:36.245871: step 53, loss 208.728, acc 0.421875
2017-03-13T04:50:14.045404: step 54, loss 297.441, acc 0.402344
2017-03-13T04:50:52.194052: step 55, loss 365.11, acc 0.332031
2017-03-13T04:51:30.185668: step 56, loss 268.598, acc 0.390625
2017-03-13T04:52:08.297215: step 57, loss 463.569, acc 0.410156
2017-03-13T04:52:46.804966: step 58, loss 790.851, acc 0.421875
2017-03-13T04:53:24.920635: step 59, loss 305.515, acc 0.386719
2017-03-13T04:54:03.391494: step 60, loss 319.676, acc 0.394531
2017-03-13T04:54:41.714109: step 61, loss 315.019, acc 0.363281
2017-03-13T04:54:53.252554: step 62, loss 327.658, acc 0.342105
2017-03-13T04:55:31.392210: step 63, loss 562.859, acc 0.300781
2017-03-13T04:56:09.584153: step 64, loss 333.671, acc 0.308594
2017-03-13T04:56:47.878877: step 65, loss 479.166, acc 0.34375
2017-03-13T04:57:26.389043: step 66, loss 453.422, acc 0.34375
2017-03-13T04:58:04.480062: step 67, loss 504.693, acc 0.308594
2017-03-13T04:58:42.996722: step 68, loss 475.311, acc 0.382812
2017-03-13T04:59:21.708645: step 69, loss 419.446, acc 0.335938
2017-03-13T05:00:00.232008: step 70, loss 456.41, acc 0.363281
2017-03-13T05:00:38.270259: step 71, loss 896.117, acc 0.363281
2017-03-13T05:01:16.471548: step 72, loss 474.767, acc 0.355469
2017-03-13T05:01:54.725439: step 73, loss 456.53, acc 0.378906
2017-03-13T05:02:32.820705: step 74, loss 602.814, acc 0.367188
2017-03-13T05:03:10.587351: step 75, loss 537.469, acc 0.355469
2017-03-13T05:03:48.983733: step 76, loss 590.922, acc 0.410156
2017-03-13T05:04:27.293452: step 77, loss 595.925, acc 0.355469
2017-03-13T05:05:05.785426: step 78, loss 685.397, acc 0.363281
2017-03-13T05:05:44.473646: step 79, loss 621.792, acc 0.371094
2017-03-13T05:06:22.476409: step 80, loss 1158.87, acc 0.3125
2017-03-13T05:07:00.828122: step 81, loss 814.057, acc 0.359375
2017-03-13T05:07:39.097135: step 82, loss 646.877, acc 0.339844
2017-03-13T05:07:50.333352: step 83, loss 931.599, acc 0.407895
2017-03-13T05:08:28.612154: step 84, loss 647.577, acc 0.378906
2017-03-13T05:09:06.812518: step 85, loss 829.198, acc 0.324219
2017-03-13T05:09:45.162953: step 86, loss 610.819, acc 0.378906
2017-03-13T05:10:23.628647: step 87, loss 748.257, acc 0.363281
2017-03-13T05:11:01.954077: step 88, loss 1220.84, acc 0.394531
2017-03-13T05:11:40.374175: step 89, loss 1290.05, acc 0.367188
2017-03-13T05:12:18.593821: step 90, loss 671.923, acc 0.277344
2017-03-13T05:12:57.102922: step 91, loss 891.622, acc 0.304688
2017-03-13T05:13:35.578465: step 92, loss 1255.32, acc 0.316406
2017-03-13T05:14:14.222532: step 93, loss 1004.84, acc 0.378906
2017-03-13T05:14:52.613130: step 94, loss 991.801, acc 0.25
2017-03-13T05:15:30.580081: step 95, loss 841.82, acc 0.375
2017-03-13T05:16:08.555042: step 96, loss 1207.36, acc 0.3125
2017-03-13T05:16:46.646729: step 97, loss 905.163, acc 0.371094
2017-03-13T05:17:25.208156: step 98, loss 1696.55, acc 0.351562
2017-03-13T05:18:03.499138: step 99, loss 1558.98, acc 0.324219

Evaluation:
2017-03-13T05:18:45.450748: step 100, loss 1395.04, acc 0.456505

2017-03-13T05:19:23.520579: step 100, loss 1616.3, acc 0.335938
2017-03-13T05:20:01.694448: step 101, loss 1765.7, acc 0.335938
2017-03-13T05:20:40.034113: step 102, loss 1394.75, acc 0.371094
2017-03-13T05:21:18.080334: step 103, loss 1004.54, acc 0.375
2017-03-13T05:21:29.582072: step 104, loss 1333.4, acc 0.394737
2017-03-13T05:22:08.278794: step 105, loss 2375.36, acc 0.34375
2017-03-13T05:22:46.785418: step 106, loss 1051.82, acc 0.324219
2017-03-13T05:23:25.113095: step 107, loss 2471.03, acc 0.328125
2017-03-13T05:24:03.580550: step 108, loss 1158.95, acc 0.257812
2017-03-13T05:24:41.857013: step 109, loss 1120.27, acc 0.238281
2017-03-13T05:25:20.169114: step 110, loss 1345.67, acc 0.289062
2017-03-13T05:25:58.138774: step 111, loss 1458.53, acc 0.335938
2017-03-13T05:26:36.217347: step 112, loss 1377.18, acc 0.34375
2017-03-13T05:27:13.961984: step 113, loss 2532.07, acc 0.34375
2017-03-13T05:27:52.584391: step 114, loss 1458.46, acc 0.34375
2017-03-13T05:28:30.784379: step 115, loss 1696.22, acc 0.359375
2017-03-13T05:29:09.790361: step 116, loss 1627.33, acc 0.398438
2017-03-13T05:29:48.243186: step 117, loss 1853.11, acc 0.300781
2017-03-13T05:30:26.600003: step 118, loss 1998.89, acc 0.3125
2017-03-13T05:31:04.758811: step 119, loss 1971.61, acc 0.335938
2017-03-13T05:31:43.203254: step 120, loss 2056.49, acc 0.367188
2017-03-13T05:32:21.360464: step 121, loss 2823.36, acc 0.355469
2017-03-13T05:32:59.342687: step 122, loss 2247.17, acc 0.378906
2017-03-13T05:33:37.869993: step 123, loss 1577.69, acc 0.40625
2017-03-13T05:34:16.325625: step 124, loss 2894.34, acc 0.371094
2017-03-13T05:34:27.596734: step 125, loss 6671.9, acc 0.276316
2017-03-13T05:35:05.984707: step 126, loss 2222.47, acc 0.398438
2017-03-13T05:35:44.461436: step 127, loss 1778.83, acc 0.230469
2017-03-13T05:36:22.209275: step 128, loss 2577.43, acc 0.382812
2017-03-13T05:37:00.920584: step 129, loss 3058.91, acc 0.328125
2017-03-13T05:37:38.926008: step 130, loss 2695.39, acc 0.371094
2017-03-13T05:38:17.265665: step 131, loss 2316.92, acc 0.324219
2017-03-13T05:38:55.622580: step 132, loss 3308.83, acc 0.378906
2017-03-13T05:39:34.052902: step 133, loss 2423.82, acc 0.460938
2017-03-13T05:40:12.479503: step 134, loss 2263.34, acc 0.441406
2017-03-13T05:40:51.244051: step 135, loss 2605.71, acc 0.308594
2017-03-13T05:41:29.602957: step 136, loss 2487.24, acc 0.335938
2017-03-13T05:42:08.888636: step 137, loss 2546.33, acc 0.320312
2017-03-13T05:42:47.130945: step 138, loss 2555.84, acc 0.421875
2017-03-13T05:43:25.278049: step 139, loss 2466.71, acc 0.402344
2017-03-13T05:44:03.845861: step 140, loss 3644.98, acc 0.375
2017-03-13T05:44:41.981669: step 141, loss 4746.04, acc 0.320312
2017-03-13T05:45:19.738899: step 142, loss 3486.43, acc 0.292969
2017-03-13T05:45:58.353813: step 143, loss 4976.86, acc 0.277344
2017-03-13T05:46:36.905715: step 144, loss 2209.77, acc 0.335938
2017-03-13T05:47:15.441876: step 145, loss 4614.9, acc 0.292969
2017-03-13T05:47:26.907400: step 146, loss 9749.88, acc 0.289474
2017-03-13T05:48:05.285489: step 147, loss 5309.81, acc 0.355469
2017-03-13T05:48:43.438652: step 148, loss 3105.11, acc 0.304688
2017-03-13T05:49:22.239024: step 149, loss 2925.3, acc 0.355469
2017-03-13T05:50:01.078360: step 150, loss 3499.42, acc 0.363281
2017-03-13T05:50:39.305329: step 151, loss 3141.5, acc 0.292969
2017-03-13T05:51:18.002051: step 152, loss 7032.62, acc 0.285156
2017-03-13T05:51:56.147927: step 153, loss 3381.99, acc 0.183594
2017-03-13T05:52:34.523821: step 154, loss 3673.15, acc 0.300781
2017-03-13T05:53:13.235600: step 155, loss 2990.61, acc 0.339844
2017-03-13T05:53:51.590888: step 156, loss 4400.73, acc 0.378906
2017-03-13T05:54:30.060945: step 157, loss 3584.59, acc 0.261719
2017-03-13T05:55:08.292169: step 158, loss 3735.79, acc 0.378906
2017-03-13T05:55:46.864472: step 159, loss 3428.75, acc 0.410156
2017-03-13T05:56:25.317116: step 160, loss 4751.99, acc 0.332031
2017-03-13T05:57:03.470510: step 161, loss 6285.22, acc 0.367188
2017-03-13T05:57:41.622194: step 162, loss 4011.77, acc 0.425781
2017-03-13T05:58:19.881022: step 163, loss 6773.11, acc 0.453125
2017-03-13T05:58:58.328752: step 164, loss 4981.79, acc 0.40625
2017-03-13T05:59:36.660978: step 165, loss 9725.56, acc 0.453125
2017-03-13T06:00:14.988463: step 166, loss 3778.72, acc 0.371094
2017-03-13T06:00:26.775515: step 167, loss 4379.04, acc 0.315789
2017-03-13T06:01:04.829935: step 168, loss 4153.7, acc 0.359375
2017-03-13T06:01:43.959642: step 169, loss 4425.5, acc 0.265625
2017-03-13T06:02:22.355950: step 170, loss 8638.71, acc 0.269531
2017-03-13T06:03:01.025365: step 171, loss 5185.92, acc 0.296875
2017-03-13T06:03:39.287060: step 172, loss 6868.08, acc 0.34375
2017-03-13T06:04:17.484141: step 173, loss 7293.12, acc 0.425781
2017-03-13T06:04:55.481018: step 174, loss 5442.6, acc 0.453125
2017-03-13T06:05:33.608516: step 175, loss 4355.97, acc 0.429688
2017-03-13T06:06:12.100442: step 176, loss 4939.97, acc 0.382812
2017-03-13T06:06:50.634496: step 177, loss 5374.14, acc 0.417969
2017-03-13T06:07:29.681824: step 178, loss 5089.46, acc 0.390625
2017-03-13T06:08:08.360451: step 179, loss 5391.23, acc 0.339844
2017-03-13T06:08:46.790666: step 180, loss 5927.54, acc 0.320312
2017-03-13T06:09:25.009111: step 181, loss 8768.97, acc 0.210938
2017-03-13T06:10:03.350301: step 182, loss 5898.42, acc 0.257812
2017-03-13T06:10:41.824767: step 183, loss 6337.99, acc 0.40625
2017-03-13T06:11:20.325126: step 184, loss 7267.83, acc 0.328125
2017-03-13T06:11:58.636006: step 185, loss 9113.09, acc 0.449219
2017-03-13T06:12:36.994123: step 186, loss 5191.58, acc 0.429688
2017-03-13T06:13:15.644534: step 187, loss 9948.77, acc 0.339844
2017-03-13T06:13:26.987942: step 188, loss 8755.92, acc 0.5
2017-03-13T06:14:05.554166: step 189, loss 5675.7, acc 0.339844
2017-03-13T06:14:44.478581: step 190, loss 8600.85, acc 0.402344
2017-03-13T06:15:22.605505: step 191, loss 6971.27, acc 0.316406
2017-03-13T06:16:00.423868: step 192, loss 5275.11, acc 0.367188
2017-03-13T06:16:38.492597: step 193, loss 7886.3, acc 0.324219
2017-03-13T06:17:16.599312: step 194, loss 5167.54, acc 0.316406
2017-03-13T06:17:54.666218: step 195, loss 13821.4, acc 0.300781
2017-03-13T06:18:33.020754: step 196, loss 8748.92, acc 0.277344
2017-03-13T06:19:11.308708: step 197, loss 14960.2, acc 0.316406
2017-03-13T06:19:49.760584: step 198, loss 9065.86, acc 0.355469
2017-03-13T06:20:27.987618: step 199, loss 7087.42, acc 0.382812

Evaluation:
2017-03-13T06:21:09.898668: step 200, loss 9709.74, acc 0.456505

2017-03-13T06:21:48.042774: step 200, loss 7307.94, acc 0.371094
2017-03-13T06:22:26.214184: step 201, loss 16738.1, acc 0.359375
2017-03-13T06:23:04.498515: step 202, loss 8483.15, acc 0.332031
2017-03-13T06:23:43.158272: step 203, loss 7613.55, acc 0.292969
2017-03-13T06:24:21.588724: step 204, loss 7212.59, acc 0.3125
2017-03-13T06:25:00.032523: step 205, loss 6832.56, acc 0.386719
2017-03-13T06:25:38.527232: step 206, loss 7208.12, acc 0.378906
2017-03-13T06:26:16.846096: step 207, loss 7863.17, acc 0.296875
2017-03-13T06:26:55.524209: step 208, loss 9544.54, acc 0.394531
2017-03-13T06:27:07.006469: step 209, loss 7121.7, acc 0.381579
2017-03-13T06:27:45.253255: step 210, loss 10019.8, acc 0.441406
2017-03-13T06:28:23.179446: step 211, loss 8136.93, acc 0.359375
2017-03-13T06:29:01.639556: step 212, loss 8303.01, acc 0.390625
2017-03-13T06:29:39.784692: step 213, loss 8966.29, acc 0.332031
2017-03-13T06:30:17.725137: step 214, loss 18294.6, acc 0.355469
2017-03-13T06:30:55.764575: step 215, loss 9673.3, acc 0.398438
2017-03-13T06:31:34.182103: step 216, loss 10711.1, acc 0.332031
2017-03-13T06:32:12.414853: step 217, loss 7440.63, acc 0.355469
2017-03-13T06:32:50.709968: step 218, loss 12140.1, acc 0.289062
2017-03-13T06:33:29.086991: step 219, loss 9791.18, acc 0.277344
2017-03-13T06:34:07.678398: step 220, loss 10113.4, acc 0.304688
2017-03-13T06:34:46.043837: step 221, loss 7982.51, acc 0.355469
2017-03-13T06:35:24.172601: step 222, loss 8696.17, acc 0.351562
2017-03-13T06:36:02.513082: step 223, loss 8029.05, acc 0.335938
2017-03-13T06:36:40.501585: step 224, loss 9059.33, acc 0.347656
2017-03-13T06:37:19.816425: step 225, loss 11285.4, acc 0.347656
2017-03-13T06:37:58.276236: step 226, loss 16713.7, acc 0.320312
2017-03-13T06:38:36.852850: step 227, loss 17299.3, acc 0.304688
2017-03-13T06:39:15.254879: step 228, loss 19300.5, acc 0.40625
2017-03-13T06:39:53.641583: step 229, loss 11411.4, acc 0.425781
2017-03-13T06:40:05.188866: step 230, loss 11085.2, acc 0.447368
2017-03-13T06:40:43.343880: step 231, loss 20418.4, acc 0.375
2017-03-13T06:41:21.621030: step 232, loss 9018.27, acc 0.429688
2017-03-13T06:41:59.877958: step 233, loss 13345.2, acc 0.480469
2017-03-13T06:42:38.463630: step 234, loss 20809.4, acc 0.453125
2017-03-13T06:43:16.906736: step 235, loss 11048, acc 0.285156
2017-03-13T06:43:55.711393: step 236, loss 10917.2, acc 0.371094
2017-03-13T06:44:33.890382: step 237, loss 9513.96, acc 0.417969
2017-03-13T06:45:12.856011: step 238, loss 13018.5, acc 0.394531
2017-03-13T06:45:51.010214: step 239, loss 8699.1, acc 0.28125
2017-03-13T06:46:29.172813: step 240, loss 9916.44, acc 0.285156
2017-03-13T06:47:07.466347: step 241, loss 10361.7, acc 0.390625
2017-03-13T06:47:45.536681: step 242, loss 11186.5, acc 0.292969
2017-03-13T06:48:23.785608: step 243, loss 13967.1, acc 0.300781
2017-03-13T06:49:01.788116: step 244, loss 11579.5, acc 0.40625
2017-03-13T06:49:39.798589: step 245, loss 18411.4, acc 0.375
2017-03-13T06:50:18.252158: step 246, loss 25668.4, acc 0.351562
2017-03-13T06:50:56.428322: step 247, loss 17226.3, acc 0.417969
2017-03-13T06:51:34.244971: step 248, loss 17675.5, acc 0.402344
2017-03-13T06:52:12.430149: step 249, loss 17687.3, acc 0.414062
2017-03-13T06:52:50.633524: step 250, loss 15969.5, acc 0.398438
2017-03-13T06:53:02.191952: step 251, loss 10644.5, acc 0.315789
2017-03-13T06:53:40.249686: step 252, loss 15504.8, acc 0.144531
2017-03-13T06:54:18.510790: step 253, loss 17540.1, acc 0.226562
2017-03-13T06:54:56.382897: step 254, loss 15068, acc 0.339844
2017-03-13T06:55:34.681370: step 255, loss 15157.3, acc 0.375
2017-03-13T06:56:12.760368: step 256, loss 11428.9, acc 0.367188
2017-03-13T06:56:50.902631: step 257, loss 15278.1, acc 0.359375
2017-03-13T06:57:29.404662: step 258, loss 20219.7, acc 0.367188
2017-03-13T06:58:07.832411: step 259, loss 16725.6, acc 0.378906
2017-03-13T06:58:46.173716: step 260, loss 15479.5, acc 0.386719
2017-03-13T06:59:24.694469: step 261, loss 10821.5, acc 0.394531
2017-03-13T07:00:03.055583: step 262, loss 13817, acc 0.40625
2017-03-13T07:00:41.156427: step 263, loss 15146.5, acc 0.390625
2017-03-13T07:01:18.610994: step 264, loss 26843.4, acc 0.351562
2017-03-13T07:01:55.612252: step 265, loss 32372.5, acc 0.320312
2017-03-13T07:02:33.712697: step 266, loss 16115.7, acc 0.335938
2017-03-13T07:03:11.411052: step 267, loss 13784.9, acc 0.371094
2017-03-13T07:03:48.991170: step 268, loss 13635.2, acc 0.351562
2017-03-13T07:04:26.877292: step 269, loss 18356, acc 0.242188
2017-03-13T07:05:04.549913: step 270, loss 28991.6, acc 0.316406
2017-03-13T07:05:42.498053: step 271, loss 24425.4, acc 0.394531
2017-03-13T07:05:53.469636: step 272, loss 23856.1, acc 0.368421
2017-03-13T07:06:30.808484: step 273, loss 22743.8, acc 0.339844
2017-03-13T07:07:08.674451: step 274, loss 15378.7, acc 0.425781
2017-03-13T07:07:46.713650: step 275, loss 18620.1, acc 0.441406
2017-03-13T07:08:24.611721: step 276, loss 13730.1, acc 0.378906
2017-03-13T07:09:02.483826: step 277, loss 21730.4, acc 0.375
2017-03-13T07:09:40.882151: step 278, loss 22477.1, acc 0.320312
2017-03-13T07:10:18.444511: step 279, loss 16212.3, acc 0.3125
2017-03-13T07:10:56.356253: step 280, loss 14550.6, acc 0.296875
2017-03-13T07:11:34.249605: step 281, loss 17754.4, acc 0.34375
2017-03-13T07:12:11.724661: step 282, loss 24750.8, acc 0.445312
2017-03-13T07:12:49.479132: step 283, loss 16982.8, acc 0.402344
2017-03-13T07:13:27.370696: step 284, loss 35233.5, acc 0.382812
2017-03-13T07:14:05.035325: step 285, loss 22593.2, acc 0.367188
2017-03-13T07:14:42.979022: step 286, loss 20261.5, acc 0.390625
2017-03-13T07:15:21.010935: step 287, loss 18076.2, acc 0.355469
2017-03-13T07:15:59.167787: step 288, loss 28081.7, acc 0.367188
2017-03-13T07:16:37.023477: step 289, loss 24581.3, acc 0.316406
2017-03-13T07:17:15.760215: step 290, loss 34356.3, acc 0.371094
2017-03-13T07:17:53.893208: step 291, loss 39831.8, acc 0.40625
2017-03-13T07:18:32.063879: step 292, loss 20500.4, acc 0.363281
2017-03-13T07:18:43.597112: step 293, loss 17972.1, acc 0.302632
2017-03-13T07:19:21.784722: step 294, loss 32948.1, acc 0.339844
2017-03-13T07:19:59.899553: step 295, loss 19136.5, acc 0.375
2017-03-13T07:20:38.573956: step 296, loss 32894.2, acc 0.3125
2017-03-13T07:21:16.676095: step 297, loss 25371.3, acc 0.328125
2017-03-13T07:21:54.839238: step 298, loss 17576.3, acc 0.378906
2017-03-13T07:22:33.262645: step 299, loss 21175.8, acc 0.398438

Evaluation:
2017-03-13T07:23:15.198205: step 300, loss 29887.3, acc 0.456505

2017-03-13T07:23:52.987723: step 300, loss 19003.5, acc 0.335938
2017-03-13T07:24:31.862163: step 301, loss 20147.1, acc 0.234375
2017-03-13T07:25:09.934784: step 302, loss 19716.1, acc 0.285156
2017-03-13T07:25:48.233350: step 303, loss 38911.8, acc 0.332031
2017-03-13T07:26:26.426816: step 304, loss 24422.5, acc 0.394531
2017-03-13T07:27:04.952364: step 305, loss 27988.3, acc 0.417969
2017-03-13T07:27:42.724648: step 306, loss 21739.1, acc 0.351562
2017-03-13T07:28:20.775629: step 307, loss 26577.1, acc 0.402344
2017-03-13T07:28:58.992641: step 308, loss 53801.5, acc 0.398438
2017-03-13T07:29:37.290605: step 309, loss 22880.6, acc 0.367188
2017-03-13T07:30:15.857189: step 310, loss 30153.3, acc 0.371094
2017-03-13T07:30:54.166275: step 311, loss 44072.4, acc 0.308594
2017-03-13T07:31:32.619020: step 312, loss 24963.3, acc 0.421875
2017-03-13T07:32:10.798453: step 313, loss 26871.7, acc 0.335938
2017-03-13T07:32:22.191499: step 314, loss 22751.2, acc 0.315789
2017-03-13T07:33:00.462634: step 315, loss 23996.7, acc 0.339844
2017-03-13T07:33:38.735154: step 316, loss 19437.5, acc 0.3125
2017-03-13T07:34:16.817664: step 317, loss 31905.6, acc 0.3125
2017-03-13T07:34:55.154268: step 318, loss 41579.7, acc 0.261719
2017-03-13T07:35:33.356518: step 319, loss 29864.2, acc 0.433594
2017-03-13T07:36:11.892640: step 320, loss 26599.9, acc 0.351562
2017-03-13T07:36:50.498565: step 321, loss 23693.7, acc 0.398438
2017-03-13T07:37:28.793680: step 322, loss 27942.6, acc 0.394531
2017-03-13T07:38:06.948356: step 323, loss 54725.6, acc 0.367188
2017-03-13T07:38:45.269730: step 324, loss 28265.5, acc 0.363281
2017-03-13T07:39:23.887592: step 325, loss 28762.8, acc 0.335938
2017-03-13T07:40:02.089075: step 326, loss 46676, acc 0.351562
2017-03-13T07:40:40.348650: step 327, loss 30513, acc 0.253906
2017-03-13T07:41:18.808028: step 328, loss 28037.4, acc 0.324219
2017-03-13T07:41:56.715417: step 329, loss 23277.9, acc 0.355469
2017-03-13T07:42:34.864487: step 330, loss 35564.2, acc 0.269531
2017-03-13T07:43:13.334719: step 331, loss 33443.3, acc 0.410156
2017-03-13T07:43:51.401547: step 332, loss 31489.4, acc 0.460938
2017-03-13T07:44:29.499176: step 333, loss 62868.6, acc 0.378906
2017-03-13T07:45:07.985658: step 334, loss 36851.8, acc 0.417969
2017-03-13T07:45:19.360836: step 335, loss 32167.1, acc 0.289474
2017-03-13T07:45:57.783803: step 336, loss 38577.9, acc 0.371094
2017-03-13T07:46:36.399422: step 337, loss 25244.2, acc 0.351562
2017-03-13T07:47:14.702281: step 338, loss 71574.1, acc 0.34375
2017-03-13T07:47:52.758251: step 339, loss 48750.5, acc 0.417969
2017-03-13T07:48:30.982264: step 340, loss 23756.5, acc 0.296875
2017-03-13T07:49:09.279483: step 341, loss 34478.1, acc 0.386719
2017-03-13T07:49:47.386212: step 342, loss 28552, acc 0.375
2017-03-13T07:50:25.669634: step 343, loss 37817.3, acc 0.304688
2017-03-13T07:51:04.375028: step 344, loss 25497, acc 0.320312
2017-03-13T07:51:42.793114: step 345, loss 24394, acc 0.316406
2017-03-13T07:52:20.563709: step 346, loss 36453.1, acc 0.292969
2017-03-13T07:52:58.741431: step 347, loss 31434.3, acc 0.316406
2017-03-13T07:53:37.158397: step 348, loss 33565.2, acc 0.242188
2017-03-13T07:54:15.447948: step 349, loss 35857, acc 0.429688
2017-03-13T07:54:53.606115: step 350, loss 47760.2, acc 0.433594
2017-03-13T07:55:32.246332: step 351, loss 49272.4, acc 0.390625
2017-03-13T07:56:10.324262: step 352, loss 33564.4, acc 0.394531
2017-03-13T07:56:48.578060: step 353, loss 46404.2, acc 0.332031
2017-03-13T07:57:27.245422: step 354, loss 32813.9, acc 0.414062
2017-03-13T07:58:05.510314: step 355, loss 81298.3, acc 0.347656
2017-03-13T07:58:16.771506: step 356, loss 19785.5, acc 0.236842
2017-03-13T07:58:55.033718: step 357, loss 45523, acc 0.359375
2017-03-13T07:59:33.524077: step 358, loss 34796.4, acc 0.394531
2017-03-13T08:00:11.897189: step 359, loss 30336.8, acc 0.347656
2017-03-13T08:00:49.957929: step 360, loss 64848.6, acc 0.410156
2017-03-13T08:01:28.201396: step 361, loss 29098.5, acc 0.417969
2017-03-13T08:02:06.466184: step 362, loss 61283.3, acc 0.335938
2017-03-13T08:02:44.768537: step 363, loss 50538.8, acc 0.242188
2017-03-13T08:03:23.156598: step 364, loss 37892.6, acc 0.367188
2017-03-13T08:04:01.152205: step 365, loss 63968, acc 0.414062
2017-03-13T08:04:39.156051: step 366, loss 41981.8, acc 0.378906
2017-03-13T08:05:17.558801: step 367, loss 36075.4, acc 0.410156
2017-03-13T08:05:55.893004: step 368, loss 32624.2, acc 0.390625
2017-03-13T08:06:34.557579: step 369, loss 37878.6, acc 0.367188
2017-03-13T08:07:12.702302: step 370, loss 84682.5, acc 0.339844
2017-03-13T08:07:50.776695: step 371, loss 56157.7, acc 0.285156
2017-03-13T08:08:28.960375: step 372, loss 41305.3, acc 0.382812
2017-03-13T08:09:07.450997: step 373, loss 40239.7, acc 0.402344
2017-03-13T08:09:45.455990: step 374, loss 38270.7, acc 0.347656
2017-03-13T08:10:24.016658: step 375, loss 32745, acc 0.320312
2017-03-13T08:11:02.139788: step 376, loss 58594.9, acc 0.183594
2017-03-13T08:11:13.548597: step 377, loss 38316.2, acc 0.368421
2017-03-13T08:11:51.564326: step 378, loss 57896.5, acc 0.414062
2017-03-13T08:12:29.793776: step 379, loss 72328.6, acc 0.371094
2017-03-13T08:13:08.207296: step 380, loss 37128.5, acc 0.417969
2017-03-13T08:13:46.176365: step 381, loss 43378.4, acc 0.386719
2017-03-13T08:14:24.161377: step 382, loss 45481.3, acc 0.34375
2017-03-13T08:15:02.247047: step 383, loss 54173.1, acc 0.371094
2017-03-13T08:15:40.630635: step 384, loss 41227.8, acc 0.289062
2017-03-13T08:16:18.101132: step 385, loss 63064, acc 0.269531
2017-03-13T08:16:56.278282: step 386, loss 45963.3, acc 0.316406
2017-03-13T08:17:34.202026: step 387, loss 47870.9, acc 0.371094
2017-03-13T08:18:12.097222: step 388, loss 62930.2, acc 0.398438
2017-03-13T08:18:50.147600: step 389, loss 42552.4, acc 0.445312
2017-03-13T08:19:28.077092: step 390, loss 42644.3, acc 0.367188
2017-03-13T08:20:06.562390: step 391, loss 110184, acc 0.3125
2017-03-13T08:20:44.660143: step 392, loss 45070.5, acc 0.296875
2017-03-13T08:21:22.963742: step 393, loss 54028, acc 0.359375
2017-03-13T08:22:01.091164: step 394, loss 34038.3, acc 0.402344
2017-03-13T08:22:38.851559: step 395, loss 47807.8, acc 0.402344
2017-03-13T08:23:16.982010: step 396, loss 46864.2, acc 0.445312
2017-03-13T08:23:55.599464: step 397, loss 75835.3, acc 0.4375
2017-03-13T08:24:06.868277: step 398, loss 39193.1, acc 0.368421
2017-03-13T08:24:44.572667: step 399, loss 53252.1, acc 0.425781

Evaluation:
2017-03-13T08:25:26.460390: step 400, loss 67451.4, acc 0.456505

2017-03-13T08:26:04.390068: step 400, loss 50453.5, acc 0.335938
2017-03-13T08:26:42.455123: step 401, loss 84893.5, acc 0.316406
2017-03-13T08:27:20.668052: step 402, loss 53231.3, acc 0.359375
2017-03-13T08:27:58.893859: step 403, loss 46676, acc 0.320312
2017-03-13T08:28:37.018684: step 404, loss 62170.4, acc 0.417969
2017-03-13T08:29:15.126638: step 405, loss 58425.5, acc 0.382812
2017-03-13T08:29:54.190982: step 406, loss 88273.2, acc 0.402344
2017-03-13T08:30:32.550586: step 407, loss 78981.8, acc 0.445312
2017-03-13T08:31:10.671998: step 408, loss 44963.6, acc 0.371094
2017-03-13T08:31:48.702939: step 409, loss 53267.3, acc 0.363281
2017-03-13T08:32:26.452676: step 410, loss 74191, acc 0.363281
2017-03-13T08:33:05.426426: step 411, loss 70138.4, acc 0.246094
2017-03-13T08:33:43.297123: step 412, loss 94860.1, acc 0.15625
2017-03-13T08:34:21.758851: step 413, loss 51067.3, acc 0.351562
2017-03-13T08:34:59.875714: step 414, loss 58813.1, acc 0.375
2017-03-13T08:35:38.611916: step 415, loss 50423.4, acc 0.367188
2017-03-13T08:36:17.437824: step 416, loss 56313.5, acc 0.371094
2017-03-13T08:36:55.971317: step 417, loss 59634.5, acc 0.359375
2017-03-13T08:37:34.042320: step 418, loss 44372.5, acc 0.402344
2017-03-13T08:37:45.576274: step 419, loss 68382.8, acc 0.210526
2017-03-13T08:38:23.801550: step 420, loss 68699.4, acc 0.34375
2017-03-13T08:39:01.720958: step 421, loss 57598.6, acc 0.367188
2017-03-13T08:39:40.234180: step 422, loss 92821, acc 0.324219
2017-03-13T08:40:18.543349: step 423, loss 73609.5, acc 0.324219
2017-03-13T08:40:56.709194: step 424, loss 52809.9, acc 0.421875
2017-03-13T08:41:34.784312: step 425, loss 102305, acc 0.421875
2017-03-13T08:42:13.645134: step 426, loss 60726.3, acc 0.40625
2017-03-13T08:42:51.758911: step 427, loss 61164.5, acc 0.429688
2017-03-13T08:43:30.164284: step 428, loss 58084.8, acc 0.421875
2017-03-13T08:44:08.516649: step 429, loss 59940.9, acc 0.347656
2017-03-13T08:44:47.189109: step 430, loss 64080.2, acc 0.3125
2017-03-13T08:45:25.830656: step 431, loss 82634.2, acc 0.335938
2017-03-13T08:46:03.676361: step 432, loss 90947.4, acc 0.324219
2017-03-13T08:46:41.529646: step 433, loss 69090.8, acc 0.40625
2017-03-13T08:47:19.797700: step 434, loss 47948.1, acc 0.441406
2017-03-13T08:47:58.720505: step 435, loss 57530.5, acc 0.339844
2017-03-13T08:48:36.544636: step 436, loss 67696.4, acc 0.296875
2017-03-13T08:49:14.883121: step 437, loss 121552, acc 0.183594
2017-03-13T08:49:53.060211: step 438, loss 61792.2, acc 0.238281
2017-03-13T08:50:30.931106: step 439, loss 76347.7, acc 0.367188
2017-03-13T08:50:42.507323: step 440, loss 77270.5, acc 0.315789
2017-03-13T08:51:20.927814: step 441, loss 54458.8, acc 0.359375
2017-03-13T08:51:59.613614: step 442, loss 87332.3, acc 0.351562
2017-03-13T08:52:38.410252: step 443, loss 57206.3, acc 0.238281
2017-03-13T08:53:16.592233: step 444, loss 131339, acc 0.269531
2017-03-13T08:53:55.018300: step 445, loss 72915.5, acc 0.320312
2017-03-13T08:54:33.542249: step 446, loss 105289, acc 0.304688
2017-03-13T08:55:11.711965: step 447, loss 73419.9, acc 0.417969

Process finished with exit code -1
